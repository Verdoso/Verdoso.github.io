<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>On (virtual) threads and pools | GreenEyed's</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Much has already been written about the new feature known as virtual threads, introduced as a preview in Java 19, and about the changes that this feature will enable. Among those, we can find that the new virtual threads are light, as opposed to the regular ones tied to native threads, so there is no need to cache or pool them. So far, so good.
Native threads are considered heavy, so you can create a limited number of them, but virtual threads are light so they are virtually, pun intended, limitless."><meta name=generator content="Hugo 0.108.0"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=/ananke/css/main.min.css><link rel=stylesheet href=/css/greeneyed.css><meta property="og:title" content="On (virtual) threads and pools"><meta property="og:description" content="Much has already been written about the new feature known as virtual threads, introduced as a preview in Java 19, and about the changes that this feature will enable. Among those, we can find that the new virtual threads are light, as opposed to the regular ones tied to native threads, so there is no need to cache or pool them. So far, so good.
Native threads are considered heavy, so you can create a limited number of them, but virtual threads are light so they are virtually, pun intended, limitless."><meta property="og:type" content="article"><meta property="og:url" content="https://www.greeneyed.org/post/virtual-threads-part-i/"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-12-02T13:45:03+00:00"><meta property="article:modified_time" content="2022-12-02T13:45:03+00:00"><meta itemprop=name content="On (virtual) threads and pools"><meta itemprop=description content="Much has already been written about the new feature known as virtual threads, introduced as a preview in Java 19, and about the changes that this feature will enable. Among those, we can find that the new virtual threads are light, as opposed to the regular ones tied to native threads, so there is no need to cache or pool them. So far, so good.
Native threads are considered heavy, so you can create a limited number of them, but virtual threads are light so they are virtually, pun intended, limitless."><meta itemprop=datePublished content="2022-12-02T13:45:03+00:00"><meta itemprop=dateModified content="2022-12-02T13:45:03+00:00"><meta itemprop=wordCount content="2119"><meta itemprop=keywords content="java,virtual threads,"><meta name=twitter:card content="summary"><meta name=twitter:title content="On (virtual) threads and pools"><meta name=twitter:description content="Much has already been written about the new feature known as virtual threads, introduced as a preview in Java 19, and about the changes that this feature will enable. Among those, we can find that the new virtual threads are light, as opposed to the regular ones tied to native threads, so there is no need to cache or pool them. So far, so good.
Native threads are considered heavy, so you can create a limited number of them, but virtual threads are light so they are virtually, pun intended, limitless."></head><body class="ma0 avenir bg-near-white"><header class="cover bg-top" style=background-image:url(https://www.greeneyed.org/uploads/loom-g85da6e78d_640.jpg)><div class=bg-black-60><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib">GreenEyed's</a><div class="flex-l items-center"><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/about/ title="About page">About</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white no-underline white-90" href=/post/ title="Posts page">Posts</a></li></ul><div class=ananke-socials><a href=https://twitter.com/greeneyed_dlj target=_blank rel=noopener class="twitter ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="Twitter link" aria-label="follow on Twitter——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167 22.283c-2.619.953-4.274 3.411-4.086 6.101l.063 1.038-1.048-.127c-3.813-.487-7.145-2.139-9.974-4.915l-1.383-1.377-.356 1.017c-.754 2.267-.272 4.661 1.299 6.271.838.89.649 1.017-.796.487-.503-.169-.943-.296-.985-.233-.146.149.356 2.076.754 2.839.545 1.06 1.655 2.097 2.871 2.712l1.027.487-1.215.021c-1.173.0-1.215.021-1.089.467.419 1.377 2.074 2.839 3.918 3.475l1.299.444-1.131.678c-1.676.976-3.646 1.526-5.616 1.568C19.775 43.256 19 43.341 19 43.405c0 .211 2.557 1.397 4.044 1.864 4.463 1.377 9.765.783 13.746-1.568 2.829-1.673 5.657-5 6.978-8.221.713-1.716 1.425-4.851 1.425-6.354.0-.975.063-1.102 1.236-2.267.692-.678 1.341-1.419 1.467-1.631.21-.403.188-.403-.88-.043-1.781.636-2.033.551-1.152-.402.649-.678 1.425-1.907 1.425-2.267.0-.063-.314.042-.671.233-.377.212-1.215.53-1.844.72l-1.131.361-1.027-.7c-.566-.381-1.361-.805-1.781-.932C39.766 21.902 38.131 21.944 37.167 22.283zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a><a href=https://github.com/Verdoso/ target=_blank rel=noopener class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="GitHub link" aria-label="follow on GitHub——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 512 512" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M256 32C132.3 32 32 134.8 32 261.7c0 101.5 64.2 187.5 153.2 217.9 11.2 2.1 15.3-5 15.3-11.1.0-5.5-.2-19.9-.3-39.1-62.3 13.9-75.5-30.8-75.5-30.8-10.2-26.5-24.9-33.6-24.9-33.6-20.3-14.3 1.5-14 1.5-14 22.5 1.6 34.3 23.7 34.3 23.7 20 35.1 52.4 25 65.2 19.1 2-14.8 7.8-25 14.2-30.7-49.7-5.8-102-25.5-102-113.5.0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8.0.0 18.8-6.2 61.6 23.5 17.9-5.1 37-7.6 56.1-7.7 19 .1 38.2 2.6 56.1 7.7 42.8-29.7 61.5-23.5 61.5-23.5 12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6.0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5.0 30.7-.3 55.5-.3 63 0 6.1 4 13.3 15.4 11C415.9 449.1 480 363.1 480 261.7 480 134.8 379.7 32 256 32z"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a><a href=https://www.linkedin.com/in/dlopezj/ target=_blank rel=noopener class="linkedin ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="LinkedIn link" aria-label="follow on LinkedIn——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 65 65" viewBox="0 0 65 65" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M50.837 48.137V36.425c0-6.275-3.35-9.195-7.816-9.195-3.604.0-5.219 1.983-6.119 3.374V27.71h-6.79c.09 1.917.0 20.427.0 20.427h6.79V36.729c0-.609.044-1.219.224-1.655.49-1.22 1.607-2.483 3.482-2.483 2.458.0 3.44 1.873 3.44 4.618v10.929H50.837zM22.959 24.922c2.367.0 3.842-1.57 3.842-3.531-.044-2.003-1.475-3.528-3.797-3.528s-3.841 1.524-3.841 3.528c0 1.961 1.474 3.531 3.753 3.531H22.959zM34 64C17.432 64 4 50.568 4 34 4 17.431 17.432 4 34 4s30 13.431 30 30c0 16.568-13.432 30-30 30zM26.354 48.137V27.71h-6.789v20.427h6.789z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a><a href=https://techhub.social/@greeneyed_dlj target=_blank rel=noopener class="mastodon ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="Mastodon link" aria-label="follow on Mastodon——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 230 230" viewBox="0 0 230 230" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M211.80683 139.0875c-3.1825 16.36625-28.4925 34.2775-57.5625 37.74875-15.16 1.80875-30.0825 3.47125-45.99875 2.74125-26.0275-1.1925-46.565-6.2125-46.565-6.2125.0 2.53375.15625 4.94625.46875 7.2025 3.38375 25.68625 25.47 27.225 46.3925 27.9425 21.115.7225 39.91625-5.20625 39.91625-5.20625l.86875 19.09s-14.77 7.93125-41.08125 9.39c-14.50875.7975-32.52375-.365-53.50625-5.91875C9.23183 213.82 1.40558 165.31125.20808 116.09125c-.36375-14.61375-.14-28.39375-.14-39.91875.0-50.33 32.97625-65.0825 32.97625-65.0825C49.67058 3.45375 78.20308.2425 107.86433.0h.72875c29.66125.2425 58.21125 3.45375 74.8375 11.09.0.0 32.97625 14.7525 32.97625 65.0825.0.0.4125 37.13375-4.6 62.915" style="fill-rule:evenodd;clip-rule:evenodd"/><path d="M65.68743 96.45938c0 9.01375-7.3075 16.32125-16.3225 16.32125-9.01375.0-16.32-7.3075-16.32-16.32125.0-9.01375 7.30625-16.3225 16.32-16.3225 9.015.0 16.3225 7.30875 16.3225 16.3225M124.52893 96.45938c0 9.01375-7.30875 16.32125-16.3225 16.32125s-16.32125-7.3075-16.32125-16.32125c0-9.01375 7.3075-16.3225 16.32125-16.3225 9.01375.0 16.3225 7.30875 16.3225 16.3225M183.36933 96.45938c0 9.01375-7.3075 16.32125-16.32125 16.32125s-16.32125-7.3075-16.32125-16.32125c0-9.01375 7.3075-16.3225 16.32125-16.3225s16.32125 7.30875 16.32125 16.3225" fill="#fff"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div></div></div></nav><div class="tc-l pv6 ph3 ph4-ns"><div class="f2 f1-l fw2 white-90 mb0 lh-title">On (virtual) threads and pools</div></div></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked">POSTS</aside><div id=sharing class="mt3 ananke-socials"><a href="https://twitter.com/share?url=https://www.greeneyed.org/post/virtual-threads-part-i/&text=On%20%28virtual%29%20threads%20and%20pools" class="ananke-social-link twitter no-underline" aria-label="share on Twitter"><span class=icon><svg style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167 22.283c-2.619.953-4.274 3.411-4.086 6.101l.063 1.038-1.048-.127c-3.813-.487-7.145-2.139-9.974-4.915l-1.383-1.377-.356 1.017c-.754 2.267-.272 4.661 1.299 6.271.838.89.649 1.017-.796.487-.503-.169-.943-.296-.985-.233-.146.149.356 2.076.754 2.839.545 1.06 1.655 2.097 2.871 2.712l1.027.487-1.215.021c-1.173.0-1.215.021-1.089.467.419 1.377 2.074 2.839 3.918 3.475l1.299.444-1.131.678c-1.676.976-3.646 1.526-5.616 1.568C19.775 43.256 19 43.341 19 43.405c0 .211 2.557 1.397 4.044 1.864 4.463 1.377 9.765.783 13.746-1.568 2.829-1.673 5.657-5 6.978-8.221.713-1.716 1.425-4.851 1.425-6.354.0-.975.063-1.102 1.236-2.267.692-.678 1.341-1.419 1.467-1.631.21-.403.188-.403-.88-.043-1.781.636-2.033.551-1.152-.402.649-.678 1.425-1.907 1.425-2.267.0-.063-.314.042-.671.233-.377.212-1.215.53-1.844.72l-1.131.361-1.027-.7c-.566-.381-1.361-.805-1.781-.932C39.766 21.902 38.131 21.944 37.167 22.283zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a><a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.greeneyed.org/post/virtual-threads-part-i/&title=On%20%28virtual%29%20threads%20and%20pools" class="ananke-social-link linkedin no-underline" aria-label="share on LinkedIn"><span class=icon><svg style="enable-background:new 0 0 65 65" viewBox="0 0 65 65" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M50.837 48.137V36.425c0-6.275-3.35-9.195-7.816-9.195-3.604.0-5.219 1.983-6.119 3.374V27.71h-6.79c.09 1.917.0 20.427.0 20.427h6.79V36.729c0-.609.044-1.219.224-1.655.49-1.22 1.607-2.483 3.482-2.483 2.458.0 3.44 1.873 3.44 4.618v10.929H50.837zM22.959 24.922c2.367.0 3.842-1.57 3.842-3.531-.044-2.003-1.475-3.528-3.797-3.528s-3.841 1.524-3.841 3.528c0 1.961 1.474 3.531 3.753 3.531H22.959zM34 64C17.432 64 4 50.568 4 34 4 17.431 17.432 4 34 4s30 13.431 30 30c0 16.568-13.432 30-30 30zM26.354 48.137V27.71h-6.789v20.427h6.789z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div><h1 class="f1 athelas mt3 mb1">On (virtual) threads and pools</h1><time class="f6 mv4 dib tracked" datetime=2022-12-02T13:45:03Z>December 2, 2022</time>
<span class="f6 mv4 dib tracked">- 10 minutes read</span>
<span class="f6 mv4 dib tracked">- 2119 words</span></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>Much has already been written about the new feature known as virtual threads, introduced as a preview in Java 19, and about the changes that this feature will enable. Among those, we can find that the <span class=highlight-yellow>new virtual threads are light</span>, as opposed to the regular ones tied to native threads, <span class=highlight-yellow>so there is no need to cache or pool them</span>. So far, so good.</p><blockquote class=remark>Native threads are considered heavy, so you can create a limited number of them, but virtual threads are light so they are virtually, pun intended, limitless.</blockquote><p>However, beware! We should not make the mistake of immediately assuming that given that there is no need to pool them, we can simply substitute the “now old-fashioned” pools of threads with a new executor service that simply spawns a new virtual thread for every task as fast as it can create them. Why? Because <span class=highlight-yellow>threads being heavy was not the only reason why we are using pools of threads</span>.</p><p>In many occasions, the reason behind a pool of threads is controlling explicitly the level of concurrency that we want to allow for a certain set of tasks. For example, to prevent a resource from being “overwhelmed” if we know it is not prepared to handle more than a certain level of simultaneous requests. Yes, it might not be the ideal approach, as we are killing two birds with one stone (controlling concurrency and preventing the creation of too many heavy native threads) but that is how many pieces of software currently do it.</p><p>With the new virtual threads, why not send all the requests anyway and let the resources handle & queue them as they see fit? Because doing so would put the control, the responsibility, and the burden at the resource. And that might not be convenient at all. The resource might not be in our control, and might simply refuse to do it, or be unable to carry those tasks; or, if it did carry them, we would lose control over how it is done, so we might not be happy with the results anyway. If the resource were in our hands, at a minimum we would need first to make sure that it is able to handle the load, or we would have to modify it so it is able to handle it.</p><blockquote class=remark>Simply spawning virtual threads puts the control, the responsibility, and the burden of handling concurrency at the resource on the other side of the task.</blockquote><p>Let’s see some examples, based on real-life code that I have had to work with, to see why <span class=highlight-yellow>a direct translation of pools of threads to simply spawning virtual threads might not be an appropriate strategy</span>.</p><ol><li><p>We have an application that uses a pool of database connections with a max number of connections: nothing new. This pool is used by some interactive fast requests and by some background jobs that periodically fill up some cache data and for that, they run some slow queries. To control the number of slow queries that are sent against the database concurrently we use a pool of threads, for two reasons: First, because sending too many of them at the same time does indeed make them slower as they interfere with each other. Second, because if we used all the connections of the database pool for the background jobs, then there would be no connection left for the interactive requests to use. Then the interactive requests would block waiting for the slow queries to finish and the latency would suffer.</p></li><li><p>There is another application where we have to update periodically some data that queries a third party service. It has to send a couple of thousand request every day but we use a pool to throttle the requests. Why? Because we know this external service is in a shared host with limited resources and sending more than handful simultaneous requests simply increases the latency. Apart of our requests, this service has a very light load, so they do not intend to overprovision it to make it able to handle our daily requests, just so we can finish a background job faster.</p></li><li><p>I worked for some time in a B2B accommodation engine, designing the API. One of the things that we had in place was an API Gateway, to control customers who would pay a fee depending on the number of requests that they were allowed to make in a given interval of time. If they controlled the number of requests sent at their side, they could decide to queue the ones over their allotted quota. If they decided to simply send all their requests along, the API Gateway would simply block the ones over their quota, which was a loss for both parties.</p></li></ol><p>Those are just a couple of examples of “not too uncommon” situations where developers use nowadays a pool of threads to control the level of concurrency. <span class=highlight-yellow>If we simply substitute pools of threads with the new VirtualThreadPerTaskExecutor, we are going to <em>DoS</em> (Denial of Service attack) some our services</span>.</p><blockquote class=remark>We have to be careful when moving to virtual threads, else we might <em>DoS</em> some of the services in use!</blockquote><p>So, <strong>does that mean we should skip virtual threads? No</strong>, what it means is that we should make sure that if we need to control the level of concurrency of our tasks, we keep being in control. That we can accomplish by using the good ol’ concurrency mechanisms that Java provides, for example: semaphores, and with a little code reuse, we can simplify the task and create a good candidate to replace our thread pools. In the next section I&rsquo;ll show an example of how we can implement such a tool.</p><h2 id=controlling-the-level-concurrency-with-virtual-threads>Controlling the level concurrency with virtual threads</h2><p>We have talked about why it is important to make the migration to virtual threads without losing control of how much load we are causing with our tasks. Now, we will explain how to do so, using the concurrency mechanisms provided by the language itself.</p><p>There are many solutions one could employ to limiting the concurrency of tasks being executed in Java, but we are going to use the class <em><a href=https://docs.oracle.com/en/java/javase/19/docs/api/java.base/java/util/concurrent/Semaphore.html>java.util.concurrent.Semaphore</a></em>, as it allows us to directly specify the number of threads we want to be able to execute a piece of code at the same time. We could have done something similar to the class <em><a href=https://github.com/openjdk/jdk/blob/master/src/java.base/share/classes/java/util/concurrent/ThreadPoolExecutor.java>java.util.concurrent.ThreadPoolExecutor</a></em> and keep the scheduled tasks in a queue, but in this case we opted for more straightforward approach, and we let all the virtual threads be spawned and simply use the semaphore to control how many of them are executing the real work at the same time. Given that virtual threads are supposed to be light, we also wanted to check it by letting them all spawn at the same time.
You can see one possible implementation that I uploaded to GitHub: <a href=https://github.com/Verdoso/jmh-throttled-virtual-threads/blob/master/src/main/java/org/greeneyed/jmh_throttled_virtual_threads/ThrottledVirtualThreadsExecutor.java>ThrottledVirtualThreadsExecutor</a></p><blockquote class=remark>Java includes mechanisms to control the concurrency of our programs, and we can apply those same mechanisms to virtual threads.</blockquote><h3 id=testing-cpu-bound-tasks>Testing CPU bound tasks</h3><p>We have already mentioned that the problem with simply spawning virtual threads without control is the danger of overwhelming other services, but what about CPU bound tasks? I wanted to test how well virtual threads were performing when stressing the CPU and see if throttling them would make any difference, so I created and uploaded a sample project to GitHub, <a href=https://github.com/Verdoso/jmh-throttled-virtual-threads>jmh-throttled-virtual-threads</a>. I used that code to simulate creating many threads to perform a CPU consuming task by executing some floating-point arithmetic (see the method <em><a href=https://github.com/Verdoso/jmh-throttled-virtual-threads/blob/master/src/main/java/org/greeneyed/jmh_throttled_virtual_threads/VirtualThreadsTester.java#L67>VirtualThreadsTester.cpuConsumingMethod</a></em>). In order for the experiment to be more reliable, I used the <a href=https://github.com/openjdk/jmh>JMH benchmarking framework.</a> as it launches the same tasks repeatedly and adds a warm up phase to prevent some of the issues that plague short-lived microbenchmarks.</p><p>I have performed the tests executing just the CPU consuming function and repeated them adding a small delay of 50ms to simulate some I/O task or other non-CPU related factor. The test launches 10.000 virtual threads to perform the calculation and the maximum level of concurrency allowed for the throttled executor is set to 30 simultaneous tasks.</p><p>After running some tests, the result are interesting:</p><h4 id=pure-cpu-function>Pure CPU function</h4><pre tabindex=0><code>Benchmark                                   Mode  Cnt      Score    Error  Units
testFreeVirtualThreadsWithLongFactor        avgt   25  21764,275 ± 58,243  ms/op
testFreeVirtualThreadsWithSmallFactor       avgt   25     34,053 ±  0,197  ms/op
testThrottledVirtualThreadsWithLongFactor   avgt   25  21759,747 ± 66,738  ms/op
testThrottledVirtualThreadsWithSmallFactor  avgt   25     34,125 ±  0,104  ms/op
</code></pre><p>In this test, you can see that there is no difference between controlling concurrency or not. In fact, if you run the tests individually, grain of salt applied, you can see that with the small factor, the level of concurrency obtained is within a single digit so the throttling never limits anything. On the other hand, when the factor is big, the CPU consumed is so huge that it becomes the limiting factor before the throttling has any chance to limit it (30 is too big). Playing with the concurrency limit and the load factor, I was unable to come up with a combo that gets an unambiguous better result when throttling, and that&rsquo;s good because it means that the JDK/OS combo are really good at handling the tasks/CPU.</p><p>But there is more</p><h4 id=cpu--small-delay-50ms-function>CPU + small delay (50ms) function</h4><pre tabindex=0><code>Benchmark                                   Mode  Cnt      Score     Error  Units
testFreeVirtualThreadsWithLongFactor        avgt   25  22036,852 ± 134,104  ms/op
testFreeVirtualThreadsWithSmallFactor       avgt   25     95,549 ±   0,062  ms/op
testThrottledVirtualThreadsWithLongFactor   avgt   25  23505,818 ±  43,687  ms/op
testThrottledVirtualThreadsWithSmallFactor  avgt   25  20617,513 ±  11,155  ms/op
</code></pre><p>Adding a small delay, 50ms, after the calculation changes things a bit. When the CPU is not the constraint (small factor), then limiting the number of simultaneous tasks via throttling makes a big difference, for the worst. Checking the logs during the experiment showed us that with the small delay, the level of concurrency without throttling gets to 10.000, remember that without the delay it would not reach two digits. Given that the concurrency limit is set to 30 in the throttling experiment, it is clear where the delay comes from</p><p>Playing with the parameters (the delay, the concurrency limit) we can see different results but the overall conclusion is that when the tasks are CPU-bound, <span class=highlight-yellow>the new virtual threads and the JDK do a very good job when we let them handle everything</span>. If we wanted to remain in control and make sure we don&rsquo;t simply consume as much CPU as available, we know now one way to do it.</p><blockquote class=remark>JDK/OS handle pretty well CPU bound tasks, but don&rsquo;t let them consume all your resources</blockquote><h2 id=some-conclusions>Some conclusions</h2><p>Taking into account that this different results happen with exactly the same code by simply changing one or two variables. It is quite clear, in case there was any doubt, that <span class=highlight-yellow>the best approach is not to follow one strategy blindly but to measure and see what will work best given the circumstances in each case</span>, and then be ready to change if those circumstances change.</p><p>A <strong>conservative approach to move to virtual threads</strong> would be to replace all the “<em>Executors.newFixedThreadPool(int)</em>” calls with something like “<em>new ThrottledVirtualThreadsExecutor(int)</em>” ones and then play with the concurrency level to see if we can handle more. If our tasks are CPU bound it is very likely that we will be able to increase the concurrency level, no more native threads limit yay!, but be careful not to consume the whole CPU if that causes issues with other parts of your application (or the GC, for example). Be also aware that if the CPU consumption is high, spawning all tasks might not give you much better results, as the CPU has not changed. At the very least, your application will be a tad lighter as virtual threads are indeed lighter than regular ones.</p><p>Remember that, as there are limited resources, there is no such thing as THE bottleneck of an application, but just the CURRENT bottleneck. If we eliminate the current one, another one will surface so don&rsquo;t let the next one in line catch you unprepared.</p><h3 id=future-experiments>Future experiments</h3><p>One could easily perform different experiments by changing the cpuConsumingMethod method and fiddling with the parameters or doing some other task, like some I/O intensive operation or querying a database. For example, using this project as a template, one could check if sending 10.000 simultaneous queries to a DB gives better performance than throttling them and, in that case, see which concurrency level get the better results). Be careful when doing this type of tests though, never in production, as you might <em>DoS</em> your DB. Monitoring the DB would be important, as your service might respond fine but might be making the DB unusable for anything else.</p><p>If you do your own tests and see some interesting results, please share them!</p><p>(Image by <a href="https://pixabay.com/users/annaer-35513/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=173821">Anna</a> from <a href="https://pixabay.com//?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=173821">Pixabay</a>)</p><ul class=pa0><li class="list di"><a href=/tags/java/ class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Java</a></li><li class="list di"><a href=/tags/virtual-threads/ class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">virtual threads</a></li></ul><div class="mt6 instapaper_ignoref"><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//greeneyed.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div><aside class="w-30-l mt6-l"><div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links"><p class="f5 b mb3">Related</p><ul class="pa0 list"><li class=mb2><a href=/post/spring-boot-json-api-versioning/>Spring Boot JSON API versioning options</a></li></ul></div></aside></article></main><footer class="bg-dark-green bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://www.greeneyed.org/>&copy; GreenEyed's 2022</a><div><div class=ananke-socials><a href=https://twitter.com/greeneyed_dlj target=_blank rel=noopener class="twitter ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="Twitter link" aria-label="follow on Twitter——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167 22.283c-2.619.953-4.274 3.411-4.086 6.101l.063 1.038-1.048-.127c-3.813-.487-7.145-2.139-9.974-4.915l-1.383-1.377-.356 1.017c-.754 2.267-.272 4.661 1.299 6.271.838.89.649 1.017-.796.487-.503-.169-.943-.296-.985-.233-.146.149.356 2.076.754 2.839.545 1.06 1.655 2.097 2.871 2.712l1.027.487-1.215.021c-1.173.0-1.215.021-1.089.467.419 1.377 2.074 2.839 3.918 3.475l1.299.444-1.131.678c-1.676.976-3.646 1.526-5.616 1.568C19.775 43.256 19 43.341 19 43.405c0 .211 2.557 1.397 4.044 1.864 4.463 1.377 9.765.783 13.746-1.568 2.829-1.673 5.657-5 6.978-8.221.713-1.716 1.425-4.851 1.425-6.354.0-.975.063-1.102 1.236-2.267.692-.678 1.341-1.419 1.467-1.631.21-.403.188-.403-.88-.043-1.781.636-2.033.551-1.152-.402.649-.678 1.425-1.907 1.425-2.267.0-.063-.314.042-.671.233-.377.212-1.215.53-1.844.72l-1.131.361-1.027-.7c-.566-.381-1.361-.805-1.781-.932C39.766 21.902 38.131 21.944 37.167 22.283zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a><a href=https://github.com/Verdoso/ target=_blank rel=noopener class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="GitHub link" aria-label="follow on GitHub——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 512 512" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M256 32C132.3 32 32 134.8 32 261.7c0 101.5 64.2 187.5 153.2 217.9 11.2 2.1 15.3-5 15.3-11.1.0-5.5-.2-19.9-.3-39.1-62.3 13.9-75.5-30.8-75.5-30.8-10.2-26.5-24.9-33.6-24.9-33.6-20.3-14.3 1.5-14 1.5-14 22.5 1.6 34.3 23.7 34.3 23.7 20 35.1 52.4 25 65.2 19.1 2-14.8 7.8-25 14.2-30.7-49.7-5.8-102-25.5-102-113.5.0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8.0.0 18.8-6.2 61.6 23.5 17.9-5.1 37-7.6 56.1-7.7 19 .1 38.2 2.6 56.1 7.7 42.8-29.7 61.5-23.5 61.5-23.5 12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6.0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5.0 30.7-.3 55.5-.3 63 0 6.1 4 13.3 15.4 11C415.9 449.1 480 363.1 480 261.7 480 134.8 379.7 32 256 32z"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a><a href=https://www.linkedin.com/in/dlopezj/ target=_blank rel=noopener class="linkedin ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="LinkedIn link" aria-label="follow on LinkedIn——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 65 65" viewBox="0 0 65 65" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M50.837 48.137V36.425c0-6.275-3.35-9.195-7.816-9.195-3.604.0-5.219 1.983-6.119 3.374V27.71h-6.79c.09 1.917.0 20.427.0 20.427h6.79V36.729c0-.609.044-1.219.224-1.655.49-1.22 1.607-2.483 3.482-2.483 2.458.0 3.44 1.873 3.44 4.618v10.929H50.837zM22.959 24.922c2.367.0 3.842-1.57 3.842-3.531-.044-2.003-1.475-3.528-3.797-3.528s-3.841 1.524-3.841 3.528c0 1.961 1.474 3.531 3.753 3.531H22.959zM34 64C17.432 64 4 50.568 4 34 4 17.431 17.432 4 34 4s30 13.431 30 30c0 16.568-13.432 30-30 30zM26.354 48.137V27.71h-6.789v20.427h6.789z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a><a href=https://techhub.social/@greeneyed_dlj target=_blank rel=noopener class="mastodon ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1" title="Mastodon link" aria-label="follow on Mastodon——Opens in a new window"><span class=icon><svg style="enable-background:new 0 0 230 230" viewBox="0 0 230 230" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M211.80683 139.0875c-3.1825 16.36625-28.4925 34.2775-57.5625 37.74875-15.16 1.80875-30.0825 3.47125-45.99875 2.74125-26.0275-1.1925-46.565-6.2125-46.565-6.2125.0 2.53375.15625 4.94625.46875 7.2025 3.38375 25.68625 25.47 27.225 46.3925 27.9425 21.115.7225 39.91625-5.20625 39.91625-5.20625l.86875 19.09s-14.77 7.93125-41.08125 9.39c-14.50875.7975-32.52375-.365-53.50625-5.91875C9.23183 213.82 1.40558 165.31125.20808 116.09125c-.36375-14.61375-.14-28.39375-.14-39.91875.0-50.33 32.97625-65.0825 32.97625-65.0825C49.67058 3.45375 78.20308.2425 107.86433.0h.72875c29.66125.2425 58.21125 3.45375 74.8375 11.09.0.0 32.97625 14.7525 32.97625 65.0825.0.0.4125 37.13375-4.6 62.915" style="fill-rule:evenodd;clip-rule:evenodd"/><path d="M65.68743 96.45938c0 9.01375-7.3075 16.32125-16.3225 16.32125-9.01375.0-16.32-7.3075-16.32-16.32125.0-9.01375 7.30625-16.3225 16.32-16.3225 9.015.0 16.3225 7.30875 16.3225 16.3225M124.52893 96.45938c0 9.01375-7.30875 16.32125-16.3225 16.32125s-16.32125-7.3075-16.32125-16.32125c0-9.01375 7.3075-16.3225 16.32125-16.3225 9.01375.0 16.3225 7.30875 16.3225 16.3225M183.36933 96.45938c0 9.01375-7.3075 16.32125-16.32125 16.32125s-16.32125-7.3075-16.32125-16.32125c0-9.01375 7.3075-16.3225 16.32125-16.3225s16.32125 7.30875 16.32125 16.3225" fill="#fff"/></svg></span><span class=new-window><svg height="8" style="enable-background:new 0 0 1000 1000" viewBox="0 0 1e3 1e3" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M598 128h298v298h-86V274L392 692l-60-60 418-418H598v-86zM810 810V512h86v298c0 46-40 86-86 86H214c-48 0-86-40-86-86V214c0-46 38-86 86-86h298v86H214v596h596z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></span></a></div></div></div></footer></body></html>